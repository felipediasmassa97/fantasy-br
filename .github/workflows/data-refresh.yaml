name: Daily Data Pipeline

on:
  schedule:
    # Run daily at 03:00 UTC
    - cron: "0 3 * * *"
  workflow_dispatch:
    inputs:
      environment:
        description: "Environment to deploy to"
        required: true
        type: choice
        options:
          - dev
          - demo
          - prod

env:
  GCP_PROJECT_ID: fantasy-br

jobs:
  load-data:
    runs-on: ubuntu-latest
    permissions:
      contents: read

    strategy:
      matrix:
        environment: ${{ github.event_name == 'schedule' && fromJSON('["dev", "demo", "prod"]') || fromJSON(format('["{0}"]', inputs.environment)) }}

    env:
      BQ_DATASET_ID: fdm${{ matrix.environment }}_fantasy_br

    steps:
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: Fetch Cartola API data
        run: |
          curl -sL "https://api.cartolafc.globo.com/atletas/mercado" -o market.json
          echo "Downloaded $(wc -c < market.json) bytes"

      - name: Extract and load players
        run: |
          YEAR=$(date +%Y)
          jq -c --arg temporada "$YEAR" '.atletas[] + {temporada: ($temporada | tonumber)}' market.json > players_all.jsonl
          API_COUNT=$(wc -l < players_all.jsonl | tr -d ' ')

          # Check if table exists
          if bq show "${GCP_PROJECT_ID}:${BQ_DATASET_ID}.raw_players_etl" > /dev/null 2>&1; then

            # Table exists, get existing season + player ID + round ID combinations
            bq query --use_legacy_sql=false --format=csv --max_rows=100000 \
              "SELECT DISTINCT CONCAT(CAST(temporada AS STRING), '_', CAST(atleta_id AS STRING), '_', CAST(rodada_id AS STRING)) as key 
               FROM \`${GCP_PROJECT_ID}.${BQ_DATASET_ID}.raw_players_etl\`" \
              | tail -n +2 > existing_keys.txt
            
            # Filter out existing combinations
            # This is done locally to avoid billing requirement on BigQuery for the filtering step
            jq -c --slurpfile keys <(jq -R -s 'split("\n") | map(select(length > 0))' existing_keys.txt) \
              'select(("\(.temporada)_\(.atleta_id)_\(.rodada_id)" | IN($keys[0][])) | not)' \
              players_all.jsonl > players_new.jsonl

          # Table does not exist, all players are new
          else
            cp players_all.jsonl players_new.jsonl
          fi

          NEW_COUNT=$(wc -l < players_new.jsonl | tr -d ' ')

          if [ "$NEW_COUNT" -gt 0 ]; then
            bq load --source_format=NEWLINE_DELIMITED_JSON --autodetect \
              --schema_update_option=ALLOW_FIELD_ADDITION \
              "${GCP_PROJECT_ID}:${BQ_DATASET_ID}.raw_players_etl" players_new.jsonl
            echo "API returned ${API_COUNT} players, inserted ${NEW_COUNT} new rows"
          else
            echo "API returned ${API_COUNT} players, no new rows to insert"
          fi

      - name: Extract and load clubs
        run: |
          jq -c '.clubes[]' market.json > clubs.jsonl
          bq load --source_format=NEWLINE_DELIMITED_JSON --autodetect --replace \
            "${GCP_PROJECT_ID}:${BQ_DATASET_ID}.raw_clubs" clubs.jsonl
          echo "Loaded $(wc -l < clubs.jsonl) clubs"

      - name: Extract and load positions
        run: |
          jq -c '.posicoes[]' market.json > positions.jsonl
          bq load --source_format=NEWLINE_DELIMITED_JSON --autodetect --replace \
            "${GCP_PROJECT_ID}:${BQ_DATASET_ID}.raw_positions" positions.jsonl
          echo "Loaded $(wc -l < positions.jsonl) positions"

      - name: Extract and load matches
        run: |
          YEAR=$(date +%Y)

          # Get current round number from the main partidas endpoint
          CURRENT_ROUND=$(curl -sL "https://api.cartolafc.globo.com/partidas" | jq '.rodada')
          echo "Current round: ${CURRENT_ROUND}"

          # Fetch all rounds from 1 to current
          > matches_all.jsonl
          for round in $(seq 1 $CURRENT_ROUND); do
            curl -sL "https://api.cartolafc.globo.com/partidas/${round}" | \
              jq -c --argjson temporada "$YEAR" --argjson rodada "$round" \
              '.partidas[] + {temporada: $temporada, rodada_id: $rodada}' >> matches_all.jsonl
          done

          TOTAL_COUNT=$(wc -l < matches_all.jsonl | tr -d ' ')
          echo "Fetched ${TOTAL_COUNT} matches across ${CURRENT_ROUND} rounds"

          # Replace all data in raw_matches
          bq load --source_format=NEWLINE_DELIMITED_JSON --autodetect --replace \
            "${GCP_PROJECT_ID}:${BQ_DATASET_ID}.raw_matches" matches_all.jsonl
          echo "Replaced raw_matches with ${TOTAL_COUNT} rows"

  run-dbt-dev:
    needs: load-data
    if: ${{ github.event_name == 'schedule' || inputs.environment == 'dev' }}
    uses: ./.github/workflows/reusable-dbt-build.yaml
    with:
      environment: dev
    secrets:
      GCP_SA_KEY: ${{ secrets.GCP_SA_KEY }}

  run-dbt-demo:
    needs: load-data
    if: ${{ github.event_name == 'schedule' || inputs.environment == 'demo' }}
    uses: ./.github/workflows/reusable-dbt-build.yaml
    with:
      environment: demo
    secrets:
      GCP_SA_KEY: ${{ secrets.GCP_SA_KEY }}

  run-dbt-prod:
    needs: load-data
    if: ${{ github.event_name == 'schedule' || inputs.environment == 'prod' }}
    uses: ./.github/workflows/reusable-dbt-build.yaml
    with:
      environment: prod
    secrets:
      GCP_SA_KEY: ${{ secrets.GCP_SA_KEY }}
