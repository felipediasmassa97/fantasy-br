name: Load Cartola Data

on:
  schedule:
    # Run daily at 03:00 UTC
    - cron: "0 3 * * *"
  workflow_dispatch:
    inputs:
      environment:
        description: "Environment to deploy to"
        required: true
        type: choice
        options:
          - dev
          - demo
          - prod

env:
  GCP_PROJECT_ID: fantasy-br

jobs:
  load-data:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      id-token: write

    strategy:
      matrix:
        environment: ${{ github.event_name == 'schedule' && fromJSON('["dev", "demo", "prod"]') || fromJSON(format('["{0}"]', inputs.environment)) }}

    env:
      BQ_DATASET_ID: fdm${{ matrix.environment }}_fantasy_br

    steps:
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: Fetch Cartola API data
        run: |
          curl -sL "https://api.cartolafc.globo.com/atletas/mercado" -o market.json
          echo "Downloaded $(wc -c < market.json) bytes"

      - name: Extract and load players
        run: |
          YEAR=$(date +%Y)
          jq -c --arg temporada "$YEAR" '.atletas[] + {temporada: ($temporada | tonumber)}' market.json > players_all.jsonl
          API_COUNT=$(wc -l < players_all.jsonl | tr -d ' ')

          # Check if table exists
          if bq show "${GCP_PROJECT_ID}:${BQ_DATASET_ID}.raw_players" > /dev/null 2>&1; then

            # Table exists, get existing season + player ID + round ID combinations
            bq query --use_legacy_sql=false --format=csv --max_rows=100000 \
              "SELECT DISTINCT CONCAT(CAST(temporada AS STRING), '_', CAST(atleta_id AS STRING), '_', CAST(rodada_id AS STRING)) as key 
               FROM \`${GCP_PROJECT_ID}.${BQ_DATASET_ID}.raw_players\`" \
              | tail -n +2 > existing_keys.txt
            
            # Filter out existing combinations
            # This is done locally to avoid billing requirement on BigQuery for the filtering step
            jq -c --slurpfile keys <(jq -R -s 'split("\n") | map(select(length > 0))' existing_keys.txt) \
              'select(("\(.temporada)_\(.atleta_id)_\(.rodada_id)" | IN($keys[0][])) | not)' \
              players_all.jsonl > players_new.jsonl

          # Table does not exist, all players are new
          else
            cp players_all.jsonl players_new.jsonl
          fi

          NEW_COUNT=$(wc -l < players_new.jsonl | tr -d ' ')

          if [ "$NEW_COUNT" -gt 0 ]; then
            bq load --source_format=NEWLINE_DELIMITED_JSON --autodetect \
              --schema_update_option=ALLOW_FIELD_ADDITION \
              "${GCP_PROJECT_ID}:${BQ_DATASET_ID}.raw_players" players_new.jsonl
            echo "API returned ${API_COUNT} players, inserted ${NEW_COUNT} new rows"
          else
            echo "API returned ${API_COUNT} players, no new rows to insert"
          fi

      - name: Extract and load clubs
        run: |
          jq -c '.clubes[]' market.json > clubs.jsonl
          bq load --source_format=NEWLINE_DELIMITED_JSON --autodetect --replace \
            "${GCP_PROJECT_ID}:${BQ_DATASET_ID}.raw_clubs" clubs.jsonl
          echo "Loaded $(wc -l < clubs.jsonl) clubs"

      - name: Extract and load positions
        run: |
          jq -c '.posicoes[]' market.json > positions.jsonl
          bq load --source_format=NEWLINE_DELIMITED_JSON --autodetect --replace \
            "${GCP_PROJECT_ID}:${BQ_DATASET_ID}.raw_positions" positions.jsonl
          echo "Loaded $(wc -l < positions.jsonl) positions"

      - name: Extract and load matches
        run: |
          curl -sL "https://api.cartolafc.globo.com/partidas" -o matches.json
          YEAR=$(date +%Y)
          ROUND=$(jq '.rodada' matches.json)

          # Add season and round_id to each match
          jq -c --arg temporada "$YEAR" --arg rodada "$ROUND" \
            '.partidas[] + {temporada: ($temporada | tonumber), rodada_id: ($rodada | tonumber)}' \
            matches.json > matches_all.jsonl

          API_COUNT=$(wc -l < matches_all.jsonl | tr -d ' ')

          # Check if table exists and filter duplicates
          if bq show "${GCP_PROJECT_ID}:${BQ_DATASET_ID}.raw_matches" > /dev/null 2>&1; then
            bq query --use_legacy_sql=false --format=csv --max_rows=10000 \
              "SELECT DISTINCT CONCAT(CAST(temporada AS STRING), '_', CAST(rodada_id AS STRING), '_', CAST(partida_id AS STRING)) as key 
               FROM \`${GCP_PROJECT_ID}.${BQ_DATASET_ID}.raw_matches\`" \
              | tail -n +2 > existing_match_keys.txt
            
            jq -c --slurpfile keys <(jq -R -s 'split("\n") | map(select(length > 0))' existing_match_keys.txt) \
              'select(("\(.temporada)_\(.rodada_id)_\(.partida_id)" | IN($keys[0][])) | not)' \
              matches_all.jsonl > matches_new.jsonl
          else
            cp matches_all.jsonl matches_new.jsonl
          fi

          NEW_COUNT=$(wc -l < matches_new.jsonl | tr -d ' ')

          if [ "$NEW_COUNT" -gt 0 ]; then
            bq load --source_format=NEWLINE_DELIMITED_JSON --autodetect \
              --schema_update_option=ALLOW_FIELD_ADDITION \
              "${GCP_PROJECT_ID}:${BQ_DATASET_ID}.raw_matches" matches_new.jsonl
            echo "API returned ${API_COUNT} matches for round ${RODADA}, inserted ${NEW_COUNT} new rows"
          else
            echo "API returned ${API_COUNT} matches for round ${RODADA}, no new rows to insert"
          fi
